#!/usr/bin/env python3

import argparse
import ast
import errno
import logging
import os
import re
import shlex
import shutil
import subprocess
import sys
import time

from collections import defaultdict, OrderedDict
from contextlib import ExitStack
from tempfile import NamedTemporaryFile
from threading import BoundedSemaphore, RLock, Thread

### PLANNED FEATURES

# TODO %``: substitute shell output rather than value of Python expressions
# TODO includes
# TODO variation on the file rule type that unconditionally runs the recipe
# but with a temp file with target, then percolates dirtiness only if the new
# file differs from the existing version - e.g. for files that depend on data
# from a database.
# TODO dependency globbing using Python expressions that evaluate to sequences,
# wrapped in %@{...}
# TODO flat producing: run the recipe of the target, touch all existing
# dependencies
# TODO optionally delete intermediate files
# TODO rename private members

### UTILITIES

def dedup(seq):
    """
    Deduplicates a sequence - courtesy of
    http://www.peterbe.com/plog/uniqifiers-benchmark
    """
    seen = set()
    seen_add = seen.add
    return [x for x in seq if not x in seen and not seen_add(x)]

def mtime(path, default=0):
    """
    Returns the mtime of the file at the specified path. Returns default if
    file does not exist. An OSError is raised if the file cannot be stat'd.
    """
    try:
        return os.stat(path).st_mtime
    except OSError as e:
        if e.errno == errno.ENOENT:
            return default
        raise e

def now():
    return time.time()

def indent(string):
    return os.linesep.join(map(lambda x : '    ' + x, string.splitlines()))

def remove_if_exists(path):
    try:
        os.remove(path)
    except OSError as e:
        if e.errno == errno.EISDIR:
            shutil.rmtree(path)
        elif e.errno == errno.ENOENT:
            pass # Mission. F***ing. Accomplished.
        else:
            raise e

def rename_if_exists(src, dst):
    try:
        os.rename(src, dst)
    except OSError as e:
        if e.errno != errno.ENOENT:
            raise e

### ERRORS

class ProduceError(Exception):

    def __init__(self, message, cause=None):
        Exception.__init__(self, message)
        self.cause = cause

### COMMANDLINE PROCESSING

def process_commandline(args=None):
    parser = argparse.ArgumentParser()
    parser.add_argument('-B', '--always-build', action='store_true',
            help="""unconditionally build all targets""")
    parser.add_argument('-f', '--file', default='produce.ini',
            help="""use FILE as a Producefile""")
    parser.add_argument('-n', '--dry-run', action='store_true', help="""print
            the commands that would be executed, but do not execute them""")
    parser.add_argument('-d', '--debug', action='store_true', help="""print
            debugging information""")
    parser.add_argument('-j', '--jobs', type=int, default=1, help="""Specifies
            the number of jobs (recipes) to run simultaneously.""")
    parser.add_argument('-u', '--pretend-up-to-date', metavar='FILE',
            action='append', default=[],
            help="""do not rebuild FILE or its dependencies (unless they are
            also depended on by other targets) even if out of date, but make
            sure that future invocations of Produce will still treat them as
            out of date by increasing the modification times of their changed
            dependencies as necessary""")
    parser.add_argument('target', nargs='*', help="""the target(s) to produce
            - if omitted, default target from Producefile is used""")
    return parser.parse_args(args)

### PRODUCEFILE PARSING

SECTIONHEAD_PATTERN = re.compile(r'^\[(.*)\]\s*$')
AVPAIR_PATTERN = re.compile(r'^(\S+?)\s*=\s*(.*)$', re.DOTALL)
VALUECONT_PATTERN = re.compile(r'^(\s)(.*)$', re.DOTALL)
COMMENT_PATTERN = re.compile(r'^\s*#.*$')
BLANKLINE_PATTERN = re.compile(r'^\s*$')

def parse_inifile(f):
    in_block = False
    current_name = None
    current_avpairs = None
    def current_section():
        return current_name, [(a, v.strip()) for (a, v) in current_avpairs]
    def extend_current_value(string):
        a, v = current_avpairs.pop()
        current_avpairs.append((a, v + string))
    for lineno, line in enumerate(f, start=1):
        match = SECTIONHEAD_PATTERN.match(line)
        if match:
            if in_block:
                yield current_section()
            in_block = True
            current_name = match.group(1)
            current_avpairs = []
            continue
        match = COMMENT_PATTERN.match(line)
        if match:
            continue
        if in_block:
            match = AVPAIR_PATTERN.match(line)
            if match:
                current_avpairs.append((match.group(1), match.group(2)))
                current_valuecont_pattern = VALUECONT_PATTERN
                continue
            if current_avpairs:
                match = current_valuecont_pattern.match(line)
                if match:
                    # Modify pattern for value continutation lines to cut off 
                    # only as much whitespace as the first continutation line
                    # starts with:
                    current_valuecont_pattern = re.compile(
                            r'^(' + match.group(1) + r')(.*)$', re.DOTALL)
                    extend_current_value(match.group(2))
                    continue
        match = BLANKLINE_PATTERN.match(line)
        if match:
            if current_avpairs:
                extend_current_value(os.linesep) 
            continue
        raise ProduceError('invalid line {} in Producefile'.format(lineno))
    if in_block:
        yield current_section()

def interpret_sections(sections):
    raw_globes = {}
    rules = []
    at_beginning = True
    for name, avpairs in sections:
        if at_beginning:
            at_beginning = False
            if name == '':
                raw_globes = OrderedDict(avpairs)
                continue
        if name == '':
            raise ProduceError('The global section (headed []) is only allowed'
                    ' at the beginning of the Producefile.')
        else:
            rules.append((section_name_to_regex(name), OrderedDict(avpairs)))
    return raw_globes, rules

### PATTERN MATCHING AND INTERPOLATION

def interpolate(string, varz, ignore_undefined=False,
        keep_escaped=False):
    logging.debug('interpolate called with varz: %s', {k: v for (k, v) in
            varz.items() if k != '__builtins__'})
    original_string = string
    result = ''
    while string:
        if string.startswith('%%'):
            if keep_escaped:
                result += '%%'
            else:
                result += '%'
            string = string[2:]
        elif string.startswith('%{'):
            # HACK: find the } that terminates the expansion by calling eval on
            # possible expressions of increasing length until we find one that
            # doesn't raise a syntax error. FIXME: things will still blow up if
            # the Python expression contains a comment that contains a }.
            start = 2
            error = None
            while '}' in string[start:]:
                logging.debug('looking for } in %s, starting at %s', string,
                        start)
                index = string.index('}', start)
                expression = string[2:index]
                try:
                    value = eval(expression, varz)
                    logging.debug('interpolating %s into %s', expression, value)
                    result += str(value)
                    string = string[index + 1:]
                    break
                except SyntaxError as e:
                    error = e
                except NameError as e:
                    if ignore_undefined:
                        result += string[:index + 1]
                        string = string[index + 1:]
                        break
                    else:
                        raise ProduceError(('name error in expression "{}" ' +
                                'in string "{}": {}').format(expression,
                                original_string, e))
                start = index + 1
            else:
                if error:
                    raise ProduceError(('{} evaluating expression "{}" in ' +
                            'pattern "{}": {}').format(error.__class__.__name__,
                            expression, original_string, error))
                else:
                    raise ProduceError(
                            'could not parse expression in string {}'.format(
                            original_string))
        elif string.startswith('%'):
            raise ProduceError('% must be followed by % or variable name in ' +
                    'curly braces in pattern {}'.format(original_string))
        else:
            result += string[:1]
            string = string[1:]
    return result

def section_name_to_regex(name, globes={}):
    if len(name) > 1 and name.startswith('/') and name.endswith('/'):
        try:
            return re.compile(name[1:-1])
        except Exception as e:
            raise ProduceError('{} in regex {}'.format(e, name))
    else:
        return produce_pattern_to_regex(name, globes)

def produce_pattern_to_regex(pattern, globes={}):
    ip = interpolate(pattern, globes, ignore_undefined=True,
            keep_escaped=True)
    regex = ''
    while ip:
        if ip.startswith('%%'):
            regex += '%'
            ip = ip[:2]
        # FIXME using the same variable twice in the pattern can be useful but
        # the re library will raise an exception
        elif ip.startswith('%{') and '}' in ip:
            # TODO check that the part between curly braces is a valid
            # Python identifier (or, eventually, expression)
            index = ip.index('}')
            variable = ip[2:index]
            regex += '(?P<' + variable + '>.*)'
            ip = ip[index + 1:]
        else:
            regex += re.escape(ip[:1])
            ip = ip[1:]
    regex += '$' # re.match doesn't enforce reaching the end by itself
    logging.debug('generated regex: %s', regex)
    return re.compile(regex)

### INSTANTIATED RULES

# An instantiated rule is represented by a dict that corresponds to one rule
# section from the Producefile, with the values already expanded. There are two
# special keys, target (the target as matched by the section header), and type,
# which must be one of file and task and defaults to file.

# TODO: Wrap these dictionaries, the following functions that operate on them
# as well as some of the dicts in Production into a Target class for a nicer
# abstraction.

def run_recipe(irule, outputs, dry_run=False):
    if not 'recipe' in irule:
        return
    target = irule['target']
    if irule['type'] == 'task':
        logging.info('running task %s', target)
    else:
        if os.path.exists(target):
            logging.info('rebuilding file %s', target)
        else:
            logging.info('building file %s', target)
    recipe = irule['recipe']
    executable = irule.get('shell', 'bash')
    if recipe.startswith('\n'):
        recipe = recipe[1:]
    print(indent(recipe)) # TODO add --s --silent --quiet option
    if not dry_run:
        # Remove old backup files, if any:
        if irule['type'] == 'file':
            backup_name = target + '~'
            remove_if_exists(backup_name)
        for output in outputs:
            backup_name = output + '~'
            remove_if_exists(backup_name)
        with NamedTemporaryFile(mode='w') as recipefile:
            recipefile.write(recipe)
            recipefile.flush()
            interrupted = False
            try:
                exit = subprocess.call([executable, recipefile.name],
                        stdin=sys.stdin)
            except KeyboardInterrupt as e:
                interrupted = True
        if interrupted or exit != 0:
            # The outputs may have been touched but since the recipe failed,
            # they probably don't have the right contents now. In order to
            # prevent future invocations of Produce from treating them as up to
            # date, we rename them, appending a tilde to the name:
            if irule['type'] == 'file':
                backup_name = target + '~'
                logging.debug('renaming %s to %s', target, backup_name)
                rename_if_exists(target, backup_name)
            for output in outputs:
                backup_name = output + '~'
                logging.debug('renaming %s to %s', output, backup_name)
                rename_if_exists(output, backup_name)
        if interrupted:
            raise ProduceError('interrupted while executing recipe')
        if exit != 0:
            raise ProduceError('recipe failed: {}'.format(recipe))
        # TODO check here if file was created for file rules, raise exception if not?

def create_irule(target, rules, globes):
    # Go through rules until a pattern matches the target:
    for pattern, avdict in rules:
        match = pattern.match(target)
        if match:
            # Dictionary representing the instantiated rule:
            result = {}
            # Dictionary for local variables:
            # TODO is the empty string a good default?
            varz = dict(globes, **match.groupdict(default = ''))
            # Special attribute: target
            result['target'] = target
            varz['target'] = target
            # Process attributes and their values:
            for attribute, value in avdict.items():
                # Remove prefix from attribute to get local variable name:
                loke = attribute.split('.')[-1]
                if loke == 'target':
                    raise ProduceError('cannot overwrite "target" attribute ' +
                            'in rule for {}'.format(target))
                # Do expansions in value:
                iv = interpolate(value, varz)
                # Attribute retains prefix:
                result[attribute] = iv
                # Local variable does not retain prefix:
                varz[loke] = iv
                # If there is a condition and it isn't met, we stop processing
                # attributes so they don't raise errors:
                if attribute == 'cond' and not ast.literal_eval(iv):
                    break
            # If there is a condition and it isn't met, go to the next rule:
            if 'cond' in result and not ast.literal_eval(result['cond']):
                logging.debug('condition %s failed, trying next rule', result['cond'])
                continue
            logging.debug('instantiated rule for target %s has varz: %s',
                    target, {k: v for (k, v) in varz.items() if k !=
                    '__builtins__'})
            result['type'] = result.get('type', 'file')
            logging.debug('target type: %s', result['type'])
            if result['type'] not in ('file', 'task'):
                raise ProduceError('unknown type {} for target {}'.format(
                    target_type, target))
            return result
        else:
            logging.debug('pattern %s did not match, trying next rule', pattern)
    if os.path.exists(target):
        # Although there is no rule to make the target, the target is a file
        # that exists, so we can use it as an ingredient.
        return {'target': target, 'type': 'file'}
    raise ProduceError('no rule to produce {}'.format(target))

def list_ddeps(irule):
    result = []
    for key, value in irule.items():
        if key.startswith('dep.'):
            result.append(value)
        elif key == 'deps':
            result.extend(shlex.split(value))
        elif key == 'depfile':
            try:
                result.extend(read_depfile(value))
            except IOError as e:
                raise ProduceError('cannot read depfile {} for target {}' \
                        .format(value, irule['target']), e)
    return result

def read_depfile(filename):
    with open(filename) as f:
        return list(map(str.strip, f))

### PRODUCTION

class BuildIfRequiredThread(Thread):

    def __init__(self, production, target):
        Thread.__init__(self)
        self.production = production
        self.target = target
        self.exception = None

    def run(self):
        try:
            self.result = self.production.build_if_required(self.target)
        except Exception as e:
            # Here we could store the stacktrace in case we want to inspect it
            # the calling thread, but currently it's not needed.
            self.exception = e

    def get_result(self):
        if self.exception:
            raise self.exception
        return self.result

class Production:

    """
    An object of this class represents one run of produce. The methods are the
    building blocks of the algorithm, the attributes represent options and
    state that changes during the production process.
    """

    def __init__(self, targets, rules, globes, dry_run, always_build, jobs,
            pretend_up_to_date):
        # General properties, passed as arguments to constructor:
        self.rules = rules # maps patterns to key-uninstantiated value maps ("rules")
        self.globes = globes
        self.dry_run = dry_run
        self.always_build = always_build
        self.pretend_up_to_date = pretend_up_to_date
        # Create locks for the building phase:
        self.fields_lock = RLock()
        self.target_lock = defaultdict(RLock)
        logging.debug('using up to %s threads', jobs)
        self.build_sema = BoundedSemaphore(jobs)
        # Initialize fields to store information about individual targets:
        self.targets = []
        self.target_outputs = {}
        self.target_irule = {} # maps targets to key-instantiated value maps ("instantiated rules")
        self.target_ddeps = {} # direct dependencies
        self.target_time = {}
        self.out_of_date_targets = set()
        self.missing_targets = set()
        self.changed_ddeps = {} # maps a dependent to a changed direct dependency that causes it to be out of date

    def add_target(self, target, beam):
        # Catch cyclic dependencies:
        if target in beam:
            raise ProduceError('cyclic dependency: {}'.format(' -> '.join(beam)
                    ))
        # Stop on encountering a target for the second time:
        if target in self.targets:
            return
        # Make instantiated rule:
        irule = create_irule(target, self.rules, self.globes)
        # Determine outputs:
        if 'outputs' in irule:
            outputs = shlex.split(irule['outputs'])
        else:
            outputs = []
        # Catch "soft cyclic dependencies":
        for output in outputs:
            if output in beam:
                raise ProduceError(
                        'cyclic dependency: {}, which has {} as output'.format(
                        ' -> '.join(beam), output))
        # The first direct dependency is the depfile, if any:
        if 'depfile' in irule:
            ddep = irule['depfile']
            # Add it recursively:
            self.add_target(ddep, beam + [target])
            # Make sure it's up to date:
            self.build_if_required(ddep) # TODO do this asynchronously like for normal targets
        # Determine other dependencies and add them recursively:
        ddeps = list_ddeps(irule)
        logging.debug('%s <- %s', target, ddeps)
        for ddep in ddeps:
            self.add_target(ddep, beam + [target])
        if ddeps:
            max_ddep_time = max([self.target_time[ddep] for ddep in ddeps])
        else:
            max_ddep_time = 0
        # Determine type, existence and time of target:
        target_type = irule['type']
        if target_type == 'task':
            missing = False
            time = 0
            logging.debug('%s is a task', target)
        else:
            missing = not os.path.exists(target)
            if missing:
                time = max_ddep_time
            else:
                time = mtime(target, 0)
                logging.debug('%s time: %s', target, time)
        logging.debug('%s missing? %s', target, missing)
        # Determine whether target is out of date, based on information about
        # dependencies, all of which are at this point guaranteed to have been
        # added:
        # 1. Tasks are always considered out of date.
        # 2. If any dependency is newer than the target, the target is out of
        #    date.
        # 3. If any dependency is out of date, so is the target.
        # 4. Consider it out of date if the always_build option is True.
        if self.always_build:
            logging.debug('%s is out of date because -B', target)
        out_of_date = target_type == 'task' \
                or any([(ddep not in self.pretend_up_to_date and \
                self.is_out_of_date(ddep)) for ddep in ddeps]) \
                or self.detect_changed_ddep(target, time, ddeps) \
                or self.always_build
        logging.debug('%s out of date? %s', target, out_of_date)
        # If we did not detect the target to be out of date but we remember it
        # is, caused by some specific newer direct dependency, we need to
        # rectify the situation by touching that dependency, thereby making it
        # out of date, and detectably so by future Produce invocations. This is
        # relevant in the second pass of adding dependencies, after the build
        # process, when we preserve ignored out-of-date statuses. Note that
        # touching a dependency does not risk introducing a new inconsistency
        # because if the dependency were out of date, we would have detected
        # this target to be out of date, too. Note also that if there is no
        # direct newer file dependency but we still remember the target as out
        # of date, we are guaranteed to detect this in a future invocation in
        # some other way - either via an intermediate dependency or because
        # there is a direct task dependency.
        if not out_of_date and target in self.changed_ddeps:
            touch(self.changed_ddeps[target], now() + 1)
        # Store information about this target:
        self.targets.append(target)
        self.target_outputs[target] = outputs
        self.target_irule[target] = irule
        self.target_ddeps[target] = ddeps
        self.target_time[target] = time
        if out_of_date:
            self.out_of_date_targets.add(target)
        if missing:
            self.missing_targets.add(target)

    def detect_changed_ddep(self, target, time, ddeps):
        for ddep in ddeps:
            if self.target_time[ddep] > time:
                self.changed_ddeps[target] = ddep
                return True
        return False

    def is_out_of_date(self, target):
        return target in self.out_of_date_targets

    def is_missing(self, target):
        return target in self.missing_targets

    def build(self, target, outputs):
        if target in self.pretend_up_to_date:
            return # TODO need to raise error somewhere if target is a file and doesn't exist
        # Build required dependencies in parallel:
        threads = []
        for ddep in self.target_ddeps[target]:
            threads.append(BuildIfRequiredThread(self, ddep))
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()
        for thread in threads:
            thread.get_result() # in order to propagate any exceptions
        # Run recipe. Semaphore limits number of parallel recipes:
        with self.build_sema:
            run_recipe(self.target_irule[target], outputs, dry_run=self.dry_run)

    def build_if_required(self, target):
        outputs = sorted(set(self.target_outputs[target]))
        with ExitStack() as stack: # TODO ExitStack only in 3.3 and above, can we use something else?
            # Acquire lock for all outputs. Order is important to avoid
            # deadlocks. Release is automatic on leaving the context of stack.
            for output in outputs:
                stack.enter_context(self.target_lock[output])
            # If the target is up-to-date and non-missing, we're done:
            with self.fields_lock:
                if not (self.is_out_of_date(target) or self.is_missing(target)):
                    return False
            # Build the target:
            self.build(target, outputs)
            # All outputs are now up-to-date and non-missing:
            with self.fields_lock:
                for output in outputs + [target]:
                    self.out_of_date_targets.discard(output)
                    self.missing_targets.discard(output)
            return True

    def produce(self, targets):
        # Phase 1: build dependency graph and gather information
        for target in targets:
            self.add_target(target, [])
        # Phase 2: build all we need to build
        threads = []
        for target in targets:
            threads.append(BuildIfRequiredThread(self, target))
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()
        if all([not thread.get_result() for thread in threads]):
            logging.info('all targets are up to date')
        # Phase 3: repeat phase 1 for the pretend targets, touching files in
        # order to preserve out-of-dateness
        self.targets = [] # circumvent add_target's deduplication
        for target in self.pretend_up_to_date:
            self.add_target(target, [])

### MAIN

def produce(args=[]):
    args = process_commandline(args)
    if args.debug:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logging.basicConfig(format='%(levelname)s: %(message)s',
            level=level)
    try:
        try:
            with open(args.file) as f:
               sections = list(parse_inifile(f))
               logging.debug('parsed sections: %s', sections)
               raw_globes, rules = interpret_sections(sections)
        except IOError as e:
            raise ProduceError('cannot read file {}'.format(args.file), e)
        globes = {}
        for att, val in raw_globes.items():
            globes[att] = interpolate(val, globes)
        # Determine targets:
        targets = args.target
        if not targets:
            if 'default' in globes:
                targets = shlex.split(globes['default'])
            else:
                raise ProduceError('Don\'t know what to produce. Specify a ' +
                        'target on the command line or a default target in ' +
                        'produce.ini.')
        # Execute prelude, with globes providing the name context:
        exec(globes.get('prelude', ''), globes)
        # Produce:
        Production(targets, rules, globes, args.dry_run, args.always_build,
                args.jobs, args.pretend_up_to_date).produce(targets)
    except KeyboardInterrupt as e:
        raise ProduceError('interrupted')

if __name__ == '__main__':
    try:
        produce(None)
    except ProduceError as e:
        logging.error(e) # FIXME prints only first line of error message???
        sys.exit(1)
