#!/usr/bin/env python3

import argparse
import ast
import errno
import logging
import os
import queue
import re
import shlex
import shutil
import subprocess
import sys
import time

from collections import defaultdict, OrderedDict
from contextlib import ExitStack
from tempfile import NamedTemporaryFile
from threading import BoundedSemaphore, RLock, Thread

### PLANNED FEATURES

# TODO change log output to be both friendlier to the human eye and more
# machine-readable. Then write unit tests that check for not doing the same
# thing twice, e.g. when two targets are outputs of the same IRule or when a
# recipe fails for a target that multiple other targets depend on. Then,
# finally, merge feat/outputs into master.
# TODO includes
# TODO variation on the file rule type that unconditionally runs the recipe
# but with a temp file with target, then percolates dirtiness only if the new
# file differs from the existing version - e.g. for files that depend on data
# from a database.
# TODO flat producing: run the recipe of the target, touch all existing
# dependencies
# TODO optionally delete intermediate files
# TODO rename private members

### UTILITIES

def have_smart_terminal():
    # courtesy of apenwarr's redo
    return (os.environ.get('TERM') or 'dumb') != 'dumb'

def mtime(path, default=0):
    """
    Returns the mtime of the file at the specified path. Returns default if
    file does not exist. An OSError is raised if the file cannot be stat'd.
    """
    try:
        return os.stat(path).st_mtime
    except OSError as e:
        if e.errno == errno.ENOENT:
            return default
        raise e

def now():
    return time.time()

def remove_if_exists(path):
    try:
        os.remove(path)
    except OSError as e:
        if e.errno == errno.EISDIR:
            shutil.rmtree(path)
        elif e.errno == errno.ENOENT:
            pass # Mission. F***ing. Accomplished.
        else:
            raise e

def rename_if_exists(src, dst):
    try:
        os.rename(src, dst)
    except OSError as e:
        if e.errno != errno.ENOENT:
            raise e

def touch(path):
    if subprocess.call(['touch', path]) != 0:
        raise ProduceError('failed to touch {}'.format(path))

def shlex_join(value):
    return ' '.join((shlex.quote(str(x)) for x in value))

### CUSTOMIZED LOGGING
# It's a pain in the neck in Python.

class TargetLogMessage():

    def __init__(self, action, target, depth):
        self.action = action
        self.target = target
        self.depth = depth

    def __str__(self):
        return '{} {}'.format(self.action, self.target)

class TargetLogFormatter(logging.Formatter):

    def __init__(self, colors):
        logging.Formatter.__init__(self)
        if colors:
            self.red    = '\x1b[31m'
            self.green  = '\x1b[32m'
            self.yellow = '\x1b[33m'
            self.bold   = '\x1b[1m'
            self.plain  = '\x1b[m'
        else:
            self.red = ''
            self.green = ''
            self.yelloe = ''
            self.bold = ''
            self.plain = ''

    def format(self, record):
        if isinstance(record.msg, TargetLogMessage):
            return '{}{}{}{}{}{}{}'.format(self.green, record.msg.action,
                    ' ' * (16 - len(record.msg.action)), self.bold,
                    ' ' * record.msg.depth, record.msg.target, self.plain)
        else:
            return logging.Formatter.format(self, record)

logger = logging.getLogger('produce')
logger.propagate = False
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
handler.setLevel(logging.INFO)
formatter = TargetLogFormatter(sys.stderr.isatty() and have_smart_terminal())
handler.setFormatter(formatter)
logger.addHandler(handler)

### ERRORS

class ProduceError(Exception):

    def __init__(self, message, cause=None):
        Exception.__init__(self, message)
        self.cause = cause

### COMMANDLINE PROCESSING

def process_commandline(args=None):
    parser = argparse.ArgumentParser()
    parser.add_argument('-B', '--always-build', action='store_true',
            help="""Unconditionally build all targets""")
    parser.add_argument('-d', '--debug', action='count', default=0,
            help="""Print debugging information. Give this option multiple
            times for more information.""")
    parser.add_argument('-f', '--file', default='produce.ini',
            help="""Use FILE as a Producefile""")
    parser.add_argument('-j', '--jobs', type=int, default=1, help="""Specifies
            the number of jobs (recipes) to run simultaneously.""")
    parser.add_argument('-n', '--dry-run', action='store_true', help="""Print
            the commands that would be executed, but do not execute them""")
    parser.add_argument('-s', '--silent', action='store_true', help="""Don't
            print the commands that are executed""")
    parser.add_argument('-u', '--pretend-up-to-date', metavar='FILE',
            action='append', default=[],
            help="""Do not rebuild FILE or its dependencies (unless they are
            also depended on by other targets) even if out of date, but make
            sure that future invocations of Produce will still treat them as
            out of date by increasing the modification times of their changed
            dependencies as necessary.""")
    parser.add_argument('target', nargs='*', help="""The target(s) to produce
            - if omitted, default target from Producefile is used""")
    return parser.parse_args(args)

### PRODUCEFILE PARSING

SECTIONHEAD_PATTERN = re.compile(r'^\[(.*)\]\s*$')
AVPAIR_PATTERN = re.compile(r'^(\S+?)\s*=\s*(.*)$', re.DOTALL)
VALUECONT_PATTERN = re.compile(r'^(\s)(.*)$', re.DOTALL)
COMMENT_PATTERN = re.compile(r'^\s*#.*$')
BLANKLINE_PATTERN = re.compile(r'^\s*$')

def parse_inifile(f):
    in_block = False
    current_name = None
    current_avpairs = None
    def current_section():
        return current_name, [(a, v.strip()) for (a, v) in current_avpairs]
    def extend_current_value(string):
        a, v = current_avpairs.pop()
        current_avpairs.append((a, v + string))
    for lineno, line in enumerate(f, start=1):
        match = SECTIONHEAD_PATTERN.match(line)
        if match:
            if in_block:
                yield current_section()
            in_block = True
            current_name = match.group(1)
            current_avpairs = []
            continue
        match = COMMENT_PATTERN.match(line)
        if match:
            continue
        if in_block:
            match = AVPAIR_PATTERN.match(line)
            if match:
                current_avpairs.append((match.group(1), match.group(2)))
                current_valuecont_pattern = VALUECONT_PATTERN
                continue
            if current_avpairs:
                match = current_valuecont_pattern.match(line)
                if match:
                    # Modify pattern for value continutation lines to cut off 
                    # only as much whitespace as the first continutation line
                    # starts with:
                    current_valuecont_pattern = re.compile(
                            r'^(' + match.group(1) + r')(.*)$', re.DOTALL)
                    extend_current_value(match.group(2))
                    continue
        match = BLANKLINE_PATTERN.match(line)
        if match:
            if current_avpairs:
                extend_current_value(os.linesep) 
            continue
        raise ProduceError('invalid line {} in Producefile'.format(lineno))
    if in_block:
        yield current_section()

def interpret_sections(sections):
    raw_globes = {}
    rules = []
    at_beginning = True
    for name, avpairs in sections:
        if at_beginning:
            at_beginning = False
            if name == '':
                raw_globes = OrderedDict(avpairs)
                continue
        if name == '':
            raise ProduceError('The global section (headed []) is only allowed'
                    ' at the beginning of the Producefile.')
        else:
            rules.append((section_name_to_regex(name), OrderedDict(avpairs)))
    return raw_globes, rules

### PATTERN MATCHING AND INTERPOLATION

def interpolate(string, varz, ignore_undefined=False,
        keep_escaped=False):
    logging.debug('interpolate called with varz: %s', {k: v for (k, v) in
            varz.items() if k != '__builtins__'})
    original_string = string
    result = ''
    while string:
        if string.startswith('%%'):
            if keep_escaped:
                result += '%%'
            else:
                result += '%'
            string = string[2:]
        elif string.startswith('%{'):
            # HACK: find the } that terminates the expansion by calling eval on
            # possible expressions of increasing length until we find one that
            # doesn't raise a syntax error. FIXME: things will still blow up if
            # the Python expression contains a comment that contains a }.
            start = 2
            error = None
            while '}' in string[start:]:
                logging.debug('looking for } in %s, starting at %s', string,
                        start)
                index = string.index('}', start)
                # Add parentheses so users don't need to add them for sequence
                # expressions:
                expression = '(' + string[2:index] + ')'
                try:
                    value = eval(expression, varz)
                    logging.debug('interpolating %s into %s', expression, value)
                    result += value_to_string(value)
                    string = string[index + 1:]
                    break
                except SyntaxError as e:
                    error = e
                except NameError as e:
                    if ignore_undefined:
                        result += string[:index + 1]
                        string = string[index + 1:]
                        break
                    else:
                        raise ProduceError(('name error in expression "{}" ' +
                                'in string "{}": {}').format(expression,
                                original_string, e))
                start = index + 1
            else:
                if error:
                    raise ProduceError(('{} evaluating expression "{}" in ' +
                            'pattern "{}": {}').format(error.__class__.__name__,
                            expression, original_string, error))
                else:
                    raise ProduceError(
                            'could not parse expression in string {}'.format(
                            original_string))
        elif string.startswith('%'):
            raise ProduceError('% must be followed by % or variable name in ' +
                    'curly braces in pattern {}'.format(original_string))
        else:
            result += string[:1]
            string = string[1:]
    return result

def value_to_string(value):
    if isinstance(value, str):
        return value
    try:
        return shlex_join(value)
    except TypeError:
        return str(value)

def section_name_to_regex(name, globes={}):
    if len(name) > 1 and name.startswith('/') and name.endswith('/'):
        try:
            return re.compile(name[1:-1])
        except Exception as e:
            raise ProduceError('{} in regex {}'.format(e, name))
    else:
        return produce_pattern_to_regex(name, globes)

def produce_pattern_to_regex(pattern, globes={}):
    ip = interpolate(pattern, globes, ignore_undefined=True,
            keep_escaped=True)
    regex = ''
    while ip:
        if ip.startswith('%%'):
            regex += '%'
            ip = ip[:2]
        # FIXME using the same variable twice in the pattern can be useful but
        # the re library will raise an exception
        elif ip.startswith('%{') and '}' in ip:
            # TODO check that the part between curly braces is a valid
            # Python identifier (or, eventually, expression)
            index = ip.index('}')
            variable = ip[2:index]
            regex += '(?P<' + variable + '>.*)'
            ip = ip[index + 1:]
        else:
            regex += re.escape(ip[:1])
            ip = ip[1:]
    regex += '$' # re.match doesn't enforce reaching the end by itself
    logging.debug('generated regex: %s', regex)
    return re.compile(regex)

### INSTANTIATED RULES

# An instantiated rule is represented by a dict that corresponds to one rule
# section from the Producefile, with the values already expanded. There are two
# special keys, target (the target as matched by the section header), and type,
# which must be one of file and task and defaults to file.

def create_irule(target, rules, globes):
    # Go through rules until a pattern matches the target:
    for pattern, avdict in rules:
        match = pattern.match(target)
        if match:
            # Dictionary representing the instantiated rule:
            result = {}
            # Dictionary for local variables:
            # TODO is the empty string a good default?
            varz = dict(globes, **match.groupdict(default = ''))
            # Special attribute: target
            result['target'] = target
            varz['target'] = target
            # Process attributes and their values:
            for attribute, value in avdict.items():
                # Remove prefix from attribute to get local variable name:
                loke = attribute.split('.')[-1]
                if loke == 'target':
                    raise ProduceError('cannot overwrite "target" attribute ' +
                            'in rule for {}'.format(target))
                # Do expansions in value:
                iv = interpolate(value, varz)
                # Attribute retains prefix:
                result[attribute] = iv
                # Local variable does not retain prefix:
                varz[loke] = iv
                # If there is a condition and it isn't met, we stop processing
                # attributes so they don't raise errors:
                if attribute == 'cond' and not ast.literal_eval(iv):
                    break
            # If there is a condition and it isn't met, go to the next rule:
            if 'cond' in result and not ast.literal_eval(result['cond']):
                logging.debug('condition %s failed, trying next rule', result['cond'])
                continue
            logging.debug('instantiated rule for target %s has varz: %s',
                    target, {k: v for (k, v) in varz.items() if k !=
                    '__builtins__'})
            result['type'] = result.get('type', 'file')
            logging.debug('target type: %s', result['type'])
            if result['type'] not in ('file', 'task'):
                raise ProduceError('unknown type {} for target {}'.format(
                    target_type, target))
            return result
        else:
            logging.debug('pattern %s did not match, trying next rule', pattern)
    if os.path.exists(target):
        # Although there is no rule to make the target, the target is a file
        # that exists, so we can use it as an ingredient.
        return {'target': target, 'type': 'file'}
    raise ProduceError('no rule to produce {}'.format(target))

def list_ddeps(irule):
    result = []
    for key, value in irule.items():
        if key.startswith('dep.'):
            result.append(value)
        elif key == 'deps':
            result.extend(shlex.split(value))
        elif key == 'depfile':
            try:
                result.extend(read_depfile(value))
            except IOError as e:
                raise ProduceError('cannot read depfile {} for target {}' \
                        .format(value, irule['target']), e)
    return result

def read_depfile(filename):
    with open(filename) as f:
        return list(map(str.strip, f))

### PRODUCTION

class Producer(Thread):

    """
    A producer is an "actor" in charge of producing a single target. It is a
    thread: you create it, you start it, and when it's done, it will put itself
    onto the queue that you passed to the constructor. From there you can get
    it and call get_result to get the result.
    """

    def __init__(self, production, target, depth=0, queue=None):
        Thread.__init__(self)
        self.production = production
        self.target = target
        self.depth = depth
        self.queue = queue
        self.exception = None

    def run(self):
        try:
            self.result = self.build_if_required()
        # Catch exceptions and store them for when the calling thread tries to
        # get the result. We could store the stacktrace, too, but currently
        # it's not needed.
        except KeyboardInterrupt as e:
            self.exception = e
        except Exception as e:
            self.exception = e
        if self.queue is not None:
            self.queue.put(self)

    def build_if_required(self):
        """
        (Re)builds the target of this producer unless it is already up to date.
        This method is mainly for checking up-to-dateness and co-ordinating
        with other Producers that are running concurrently. The core work is
        delegated to _build.
        """
        # We lock all side outputs. We also lock the target iff we have a
        # recipe for it:
        outputs = set(self.production.target_outputs[self.target])
        lockables = set(outputs)
        lockables.discard(self.target)
        if self.production.target_irule[self.target].get('recipe'):
            lockables.add(self.target)
        lockables = sorted(lockables) # sort to prevent deadlock
        with ExitStack() as stack: # TODO ExitStack only in 3.3 and above, can we use something else?
            # Acquire locks. Release is automatic on leaving the context of
            # stack.
            for lockable in lockables:
                if self.production.debug >= 2:
                    logging.info('locking %s', lockable)
                stack.enter_context(self.production.target_lock[lockable])
                if self.production.debug >= 2:
                    logging.info('locked %s', lockable)
            # Check if the target still needs to be built:
            with self.production.fields_lock:
                # If the target is up-to-date and non-missing, we're done:
                if not (self.production.is_out_of_date(self.target) or \
                        self.production.is_missing(self.target)):
                    return False
                # If building this target encountered an exception before, we
                # also raise it, in case this thread is joined earlier than the
                # one which originally raised it:
                if self.target in self.production.target_exception:
                    raise self.production.target_exception[self.target]
            # Build the target:
            try:
                self._build()
            except Exception as e:
                with self.production.fields_lock:
                    self.production.target_exception[self.target] = e
                raise e
            # All outputs are now up-to-date and non-missing:
            with self.production.fields_lock:
                for output in outputs | set((self.target,)):
                    self.production.out_of_date_targets.discard(output)
                    self.production.missing_targets.discard(output)
        return True

    def _build(self):
        """
        If it has been decided that a target is out of date, this is the
        method that (re)builds it, unless we use the -u feature and pretend the
        target is up to date. As a prerequisite, the method first makes sure
        all direct dependencies are up to date by spawning and joining
        corresponding Producers.
        """
        if self.target in self.production.pretend_up_to_date:
            return # TODO need to raise error somewhere if target is a file and doesn't exist
        # Build required dependencies in parallel:
        q = queue.Queue()
        for ddep in self.production.target_ddeps[self.target]:
            Producer(self.production, ddep, depth=self.depth + 1, queue=q
                    ).start()
        # Wait until all producers are finished:
        for _ in self.production.target_ddeps[self.target]:
            q.get().get_result() # get result so exceptions are propagated
        # Run recipe. Semaphore limits number of parallel recipes:
        with self.production.build_sema:
            self._run_recipe()

    def _run_recipe(self):
        """
        This is where it really happens: the recipe for a target is run, and we
        tell the use about it.
        """
        irule = self.production.target_irule[self.target]
        if not 'recipe' in irule:
            return
        target = irule['target']
        if irule['type'] == 'task':
            logger.info(TargetLogMessage('running task', target, self.depth))
        else:
            if os.path.exists(target):
                logger.info(TargetLogMessage('rebuilding file', target,
                        self.depth))
            else:
                logger.info(TargetLogMessage('building file', target,
                        self.depth))
        recipe = irule['recipe']
        executable = irule.get('shell', 'bash')
        if recipe.startswith('\n'):
            recipe = recipe[1:]
        if not self.production.silent:
            print(recipe)
        if not self.production.dry_run:
            # Remove old backup files, if any:
            if irule['type'] == 'file':
                backup_name = target + '~'
                remove_if_exists(backup_name)
            for output in self.production.target_outputs[target]:
                backup_name = output + '~'
                remove_if_exists(backup_name)
            with NamedTemporaryFile(mode='w') as recipefile:
                recipefile.write(recipe)
                recipefile.flush()
                # Mark the files we are about to create/update as incomplete
                # so cleanup deals with them in case of an error:
                with self.production.fields_lock:
                    if irule['type'] == 'file':
                        self.production.incomplete_files.add(target)
                    for output in self.production.target_outputs[target]:
                        self.production.incomplete_files.add(output)
                    # Start running the recipe:
                    process = subprocess.Popen([executable, recipefile.name],
                            stdin=sys.stdin)
                    self.production.processes.add(process)
                # Wait for recipe to finish:
                exit = process.wait()
            if exit != 0:
                raise ProduceError('recipe failed: {}'.format(recipe))
            # Files were successfully created/updated and are no longer
            # incomplete:
            with self.production.fields_lock:
                if irule['type'] == 'file':
                    self.production.incomplete_files.discard(target)
                for output in self.production.target_outputs[target]:
                    self.production.incomplete_files.discard(output)
            # TODO check here if file was created for file rules, raise exception if not?  
    
    def get_result(self):
        """
        Returns True if the target was (re)built, False if the target was
        already update. Raises a ProduceError if something went wrong.
        """
        if self.exception:
            raise self.exception
        return self.result

class Production:

    """
    An object of this class represents one run of produce. The methods are the
    building blocks of the algorithm, the attributes represent options and
    state that changes during the production process.
    """

    def __init__(self, targets, rules, globes, dry_run, always_build, jobs,
            pretend_up_to_date, silent, debug):
        # General properties, passed as arguments to constructor:
        self.rules = rules # maps patterns to key-uninstantiated value maps ("rules")
        self.globes = globes
        self.dry_run = dry_run
        self.always_build = always_build
        self.pretend_up_to_date = pretend_up_to_date
        self.silent = silent
        self.debug = debug
        # Create locks for the building phase:
        self.fields_lock = RLock()
        self.target_lock = defaultdict(RLock)
        logging.debug('using up to %s threads', jobs)
        self.build_sema = BoundedSemaphore(jobs)
        # Initialize fields to store information about individual targets:
        self.targets = []
        self.target_outputs = {}
        self.target_irule = {} # maps targets to key-instantiated value maps ("instantiated rules")
        self.target_ddeps = {} # direct dependencies
        self.target_time = {}
        self.out_of_date_targets = set()
        self.missing_targets = set()
        # Initialize fields to store information for cleanup:
        self.incomplete_files = set()
        self.processes = set()
        # If a recipe for a target fails, store the exception for the benefit
        # of other threads that need the same target. Instead of trying to
        # build it again, they can just raise the existing exception.
        self.target_exception = {}
        # Maps a dependent to a changed direct dependency that causes it to be
        # out of date:
        self.changed_ddeps = {}

    def add_target(self, target, beam):
        # Catch cyclic dependencies:
        if target in beam:
            raise ProduceError('cyclic dependency: {}'.format(' -> '.join(beam)
                    ))
        # Stop on encountering a target for the second time:
        if target in self.targets:
            return
        # Make instantiated rule:
        irule = create_irule(target, self.rules, self.globes)
        # Store it directly (we need it for spotting harmless soft cycles):
        self.target_irule[target] = irule
        # Determine outputs:
        if 'outputs' in irule:
            outputs = shlex.split(irule['outputs'])
        else:
            outputs = []
        # Catch "soft cyclic dependencies". We don't want to allow a target to
        # depend on a rule that will produce it as a side output: the target
        # would be built twice by the same production, which is crazy and
        # probably indicates a bug. There's an exception though: if the target
        # has an empty recipe, it may depend on a target that produces the
        # first target as a side output. This is useful because it allows
        # dependencies on side outputs: they can then declare which target to
        # produce to get the side output.
        for output in outputs:
            if output in beam and self.target_irule[output].get('recipe'):
                raise ProduceError(
                        'cyclic dependency: {}, which has {} as output'.format(
                        ' -> '.join(beam + [target]), output))
        # The first direct dependency is the depfile, if any:
        if 'depfile' in irule:
            ddep = irule['depfile']
            # Add it recursively:
            self.add_target(ddep, beam + [target])
            # Make sure it's up to date:
            Producer(self, ddep).build_if_required() # TODO do this asynchronously like for normal targets
        # Determine other dependencies and add them recursively:
        ddeps = list_ddeps(irule)
        if self.debug >= 1:
            logging.info('%s <- %s', target, ddeps)
        for ddep in ddeps:
            self.add_target(ddep, beam + [target])
        if ddeps:
            max_ddep_time = max([self.target_time[ddep] for ddep in ddeps])
        else:
            max_ddep_time = 0
        # Determine type, existence and time of target:
        target_type = irule['type']
        if target_type == 'task':
            missing = False
            time = 0
            logging.debug('%s is a task', target)
        else:
            missing = not os.path.exists(target)
            if missing:
                time = max_ddep_time
            else:
                time = mtime(target, 0)
                logging.debug('%s time: %s', target, time)
        logging.debug('%s missing? %s', target, missing)
        # Determine whether target is out of date, based on information about
        # dependencies, all of which are at this point guaranteed to have been
        # added:
        # 1. Tasks are always considered out of date.
        # 2. If any dependency is newer than the target, the target is out of
        #    date.
        # 3. If any dependency is out of date, so is the target.
        # 4. Consider it out of date if the always_build option is True.
        if self.always_build:
            logging.debug('%s is out of date because -B', target)
        out_of_date = target_type == 'task' \
                or any([(ddep not in self.pretend_up_to_date and \
                self.is_out_of_date(ddep)) for ddep in ddeps]) \
                or self.detect_changed_ddep(target, time, ddeps) \
                or self.always_build
        logging.debug('%s out of date? %s', target, out_of_date)
        # If we did not detect the target to be out of date but we remember it
        # is, caused by some specific newer direct dependency, we need to
        # rectify the situation by touching that dependency, thereby making it
        # out of date, and detectably so by future Produce invocations. This is
        # relevant in the second pass of adding dependencies, after the build
        # process, when we preserve ignored out-of-date statuses. Note that
        # touching a dependency does not risk introducing a new inconsistency
        # because if the dependency were out of date, we would have detected
        # this target to be out of date, too. Note also that if there is no
        # direct newer file dependency but we still remember the target as out
        # of date, we are guaranteed to detect this in a future invocation in
        # some other way - either via an intermediate dependency or because
        # there is a direct task dependency.
        if not out_of_date and target in self.changed_ddeps:
            touch(self.changed_ddeps[target], now() + 1)
        # Store information about this target:
        self.targets.append(target)
        self.target_outputs[target] = outputs
        self.target_ddeps[target] = ddeps
        self.target_time[target] = time
        if out_of_date:
            self.out_of_date_targets.add(target)
        if missing:
            self.missing_targets.add(target)

    def detect_changed_ddep(self, target, time, ddeps):
        for ddep in ddeps:
            if self.target_time[ddep] > time:
                self.changed_ddeps[target] = ddep
                return True
        return False

    def is_out_of_date(self, target):
        return target in self.out_of_date_targets

    def is_missing(self, target):
        return target in self.missing_targets

    def produce(self, targets):
        try:
            # Phase 1: build dependency graph and gather information
            for target in targets:
                self.add_target(target, [])
            # Phase 2: build all we need to build
            q = queue.Queue()
            for target in targets:
                Producer(self, target, queue=q).start()
            all_up_to_date = True
            for _ in targets:
                all_up_to_date = not q.get().get_result() and all_up_to_date
            if all_up_to_date:
                logging.info('all targets are up to date')
        finally:
            # Phase 3: clean up
            for process in self.processes:
                try:
                    process.kill()
                except ProcessLookupError:
                    pass
            for output in self.incomplete_files:
                backup_name = output + '~'
                logging.debug('renaming %s to %s', output, backup_name)
                rename_if_exists(output, backup_name)
            # Phase 4: repeat phase 1 for the pretend targets, touching files
            # in order to preserve out-of-dateness
            self.targets = [] # circumvent add_target's deduplication
            for target in self.pretend_up_to_date:
                self.add_target(target, [])

### MAIN

def produce(args=[]):
    args = process_commandline(args)
    if args.debug > 1:
        level = logging.DEBUG
    else:
        level = logging.INFO
    logging.basicConfig(format='%(levelname)s: %(message)s',
            level=level)
    try:
        with open(args.file) as f:
           sections = list(parse_inifile(f))
           logging.debug('parsed sections: %s', sections)
           raw_globes, rules = interpret_sections(sections)
    except IOError as e:
        raise ProduceError('cannot read file {}'.format(args.file), e)
    globes = {}
    for att, val in raw_globes.items():
        globes[att] = interpolate(val, globes)
    # Determine targets:
    targets = args.target
    if not targets:
        if 'default' in globes:
            targets = shlex.split(globes['default'])
        else:
            raise ProduceError('Don\'t know what to produce. Specify a ' +
                    'target on the command line or a default target in ' +
                    'produce.ini.')
    # Execute prelude, with globes providing the name context:
    exec(globes.get('prelude', ''), globes)
    # Produce:
    Production(targets, rules, globes, args.dry_run, args.always_build,
            args.jobs, args.pretend_up_to_date, args.silent, args.debug
            ).produce(targets)

if __name__ == '__main__':
    try:
        produce(None)
    except ProduceError as e:
        logging.error(e) # FIXME prints only first line of error message???
        sys.exit(1)
